import time, json
from confluent_kafka import Consumer
from monitor import ResourceMonitorThread  # Äo CPU/RAM song song

NUM_EXPECTED = 1000  # ğŸ‘ˆ Sá»­a láº¡i sá»‘ lÆ°á»£ng cáº§n nháº­n

# Khá»Ÿi táº¡o Kafka Consumer
c = Consumer({
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'simple-consumer-group',
    'auto.offset.reset': 'earliest',
})
c.subscribe(['sos'])

print(f"ğŸ“¥ Äang láº¯ng nghe {NUM_EXPECTED:,} message... (Ctrl+C Ä‘á»ƒ thoÃ¡t)")

received_count = 0
total_latency_ms = 0
start = time.time()

monitor = ResourceMonitorThread(interval=0.2)
monitor.start()

try:
    while received_count < NUM_EXPECTED:
        msg = c.poll(1.0)
        if msg is None:
            continue
        if msg.error():
            print(f"âŒ {msg.error()}")
            continue

        now = time.time()
        try:
            data = json.loads(msg.value().decode())
            latency_ms = (now - data['sent_at']) * 1000
            total_latency_ms += latency_ms
            received_count += 1

            if received_count % 100 == 0:
                print(f"ğŸ“¨ ÄÃ£ nháº­n: {received_count:,} messages")
        except Exception as e:
            print(f"âŒ Lá»—i Ä‘á»c message: {e}")

except KeyboardInterrupt:
    print("\nğŸ›‘ ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng.")
finally:
    c.close()
    monitor.stop()
    monitor.join()
    end = time.time()

    print("\nğŸ“Š Tá»•ng káº¿t:")
    print(f"âœ… Tá»•ng nháº­n: {received_count:,} messages")

    duration_ms = (end - start) * 1000
    print(f"â±ï¸ Thá»i gian thá»±c táº¿: {duration_ms:.2f} ms")

    if received_count > 0:
        avg_latency = total_latency_ms / received_count
        print(f"ğŸ“ˆ Äá»™ trá»… trung bÃ¬nh: {avg_latency:.2f} ms")

    avg_cpu, avg_ram = monitor.get_average_usage()
    print(f"ğŸ“Š CPU trung bÃ¬nh (monitor): {avg_cpu:.2f}% | RAM trung bÃ¬nh: {avg_ram:.2f} MB")
